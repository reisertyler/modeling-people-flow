{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Author: Tyler Reiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsnmf\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from src.python.processor.data_processor import DataReader, BuildingProcessor\n",
    "data_reader         = DataReader(start_date=datetime(2019,9,1),\n",
    "                                 end_date=datetime(2019,12,16))\n",
    "processor           = BuildingProcessor(data_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.python.processor.data_processor import DataReader, BuildingProcessor\n",
    "from datetime import datetime\n",
    "data_reader     = DataReader(start_date=datetime(2021,2,10),end_date=datetime(2021,2,15))\n",
    "data_processor  = BuildingProcessor(data_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file found with the name ./AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name /AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name d/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name a/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name t/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name a/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name /AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name i/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name n/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name p/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name u/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name t/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name /AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name W/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name i/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name F/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name i/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name D/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name a/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name t/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name a/AERO_Extracted_Data_8-16-2019.csv.\n",
      "No file found with the name /AERO_Extracted_Data_8-16-2019.csv.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Eduroam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/tylerreiser/Desktop/dev/Smart-Campus/DataDecompositions.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/tylerreiser/Desktop/dev/Smart-Campus/DataDecompositions.ipynb#W0sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplot_multiple()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/tylerreiser/Desktop/dev/Smart-Campus/DataDecompositions.ipynb#W0sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m data_processor  \u001b[39m=\u001b[39m dp\u001b[39m.\u001b[39mDataProcessor(csv_directory   \u001b[39m=\u001b[39m   \u001b[39m'\u001b[39m\u001b[39m./data/input/WiFiData/\u001b[39m\u001b[39m'\u001b[39m, building_id    \u001b[39m=\u001b[39m  \u001b[39m'\u001b[39m\u001b[39mAERO\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/tylerreiser/Desktop/dev/Smart-Campus/DataDecompositions.ipynb#W0sZmlsZQ%3D%3D?line=187'>188</a>\u001b[0m data_visualizer \u001b[39m=\u001b[39m DecompositionVisualizer(data_processor)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/tylerreiser/Desktop/dev/Smart-Campus/DataDecompositions.ipynb#W0sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m data_visualizer\u001b[39m.\u001b[39mplot_multiple()\n",
      "\u001b[1;32m/Users/tylerreiser/Desktop/dev/Smart-Campus/DataDecompositions.ipynb Cell 1\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tylerreiser/Desktop/dev/Smart-Campus/DataDecompositions.ipynb#W0sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecomp \u001b[39m=\u001b[39m DecompositionMethods()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tylerreiser/Desktop/dev/Smart-Campus/DataDecompositions.ipynb#W0sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspecial_buildings \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAERO\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mATLS\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mC4C\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLIBR\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mUMC\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWLAW\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mREC\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBCAPA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBCAPB\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tylerreiser/Desktop/dev/Smart-Campus/DataDecompositions.ipynb#W0sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m data_processor\u001b[39m.\u001b[39;49mprocess_all_buildings()[\u001b[39m'\u001b[39;49m\u001b[39mEduroam\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Eduroam'"
     ]
    }
   ],
   "source": [
    "class DecompositionMethods:\n",
    "    \"\"\"\n",
    "    A class to process SVD, TruncatedSVD, tsnmf, and NMF methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=15):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def apply_svd(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "        U, s, VT = np.linalg.svd(data_2d)\n",
    "        S = np.zeros(data_2d.shape)\n",
    "        S[:self.n_components, :self.n_components] = np.diag(s[:self.n_components])\n",
    "        W = U @ S\n",
    "        H = VT\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "        return W, H, objective\n",
    "        \n",
    "    def apply_truncated_svd(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "        model = TruncatedSVD(n_components=self.n_components)\n",
    "        model.fit(data_2d)\n",
    "        W = model.transform(data_2d)\n",
    "        H = model.components_\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "        return W, H, objective\n",
    "    \n",
    "    def apply_nmf_sklearn(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "        model = NMF(n_components=self.n_components, init='random', random_state=0)\n",
    "        W = model.fit_transform(data_2d)\n",
    "        H = model.components_\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "        return W, H, objective\n",
    "\n",
    "    def apply_tsnmf(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "        model = tsnmf.smoothNMF(n_components=self.n_components)\n",
    "        model.fit(data_2d)\n",
    "        W = model.W\n",
    "        H = model.H\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "        return W, H, objective\n",
    "\n",
    "class DecompositionVisualizer:\n",
    "\n",
    "    def __init__(self, data_processor):\n",
    "        self.decomp = DecompositionMethods()\n",
    "        self.special_buildings = ['AERO', 'ATLS', 'C4C', 'LIBR', 'UMC', 'WLAW', 'REC', 'BCAPA', 'BCAPB']\n",
    "        self.data = data_processor.process_all_buildings()\n",
    "\n",
    "    def data_matrix(self, building):\n",
    "        df  =   self.data[building]\n",
    "        df['datetime'] = pd.to_datetime( df['datetime'])\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        df['date'] = df.index.date\n",
    "        df['time'] = df.index.time\n",
    "        df['hours'] = df[ 'time'].apply(lambda t: (t.hour*3600 + t.minute*60 + t.second)/3600)\n",
    "        df['week'] = df.index.isocalendar().week\n",
    "        df['minutes'] = df.index.dayofweek * 24 * 60 + df['time'].apply(lambda t: t.hour * 60 + t.minute)\n",
    "        df = df.groupby(['week', 'minutes'])['devicecount'].mean().unstack()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def plot_and_save(self, data_matrix, building):\n",
    "        print(f\"Building: {building}\")\n",
    "        start_week = data_matrix.index.get_level_values('week').min()\n",
    "        end_week = data_matrix.index.get_level_values('week').max()\n",
    "        folder_name_date = f'Week {start_week} to {end_week}'\n",
    "        building_title = building + f\": Week {start_week} to {end_week}\"\n",
    "        \n",
    "        fig = plt.figure(figsize=(14,20))\n",
    "        gs = gridspec.GridSpec(3, 3, height_ratios=[1,0.5,0.5]) \n",
    "\n",
    "        plt.suptitle(building_title, fontsize=14, fontweight='bold')\n",
    "\n",
    "        ax0 = plt.subplot(gs[0,:])\n",
    "        ax1 = plt.subplot(gs[1,0])\n",
    "        ax2 = plt.subplot(gs[2,0])\n",
    "        ax3 = plt.subplot(gs[1,1])\n",
    "        ax4 = plt.subplot(gs[2,1])\n",
    "        ax5 = plt.subplot(gs[1,2])\n",
    "        ax6 = plt.subplot(gs[2,2])\n",
    "\n",
    "        for week, weekData in data_matrix.iterrows():\n",
    "            ax0.plot(weekData.index / 1440, weekData, label=week, color='blue')\n",
    "\n",
    "        ax0.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax0.xaxis.set_minor_locator(ticker.MultipleLocator(1/24))\n",
    "        ax0.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'Day {int(x)}'))\n",
    "        \n",
    "        ax0.set_xlim([0, 7])\n",
    "        ax0.set_ylim([0, data_matrix.max().max()])\n",
    "        ax0.set_xlabel('Day of Week')\n",
    "        ax0.set_ylabel('Device Count')\n",
    "        ax0.set_title('ORIGINAL DATA - Device Count per day')\n",
    "        ax0.legend()\n",
    "\n",
    "        W_tsnmf, H_tsnmf, _ = self.decomp.apply_tsnmf(data_matrix)\n",
    "\n",
    "        ax1.plot(W_tsnmf[:,:5])\n",
    "        ax1.set_title('W - TSNMF (first 5 components)')\n",
    "        ax2.plot(H_tsnmf[:5,:].T)\n",
    "        ax2.set_title('H - TSNMF (first 5 components)')\n",
    "\n",
    "        ax3.plot(W_tsnmf[:,5:10])\n",
    "        ax3.set_title('W - TSNMF (next 5 components)')\n",
    "        ax4.plot(H_tsnmf[5:10,:].T)\n",
    "        ax4.set_title('H - TSNMF (next 5 components)')\n",
    "\n",
    "        ax5.plot(W_tsnmf[:,10:])\n",
    "        ax5.set_title('W - TSNMF (last 5 components)')\n",
    "        ax6.plot(H_tsnmf[10:,:].T)\n",
    "        ax6.set_title('H - TSNMF (last 5 components)')\n",
    "\n",
    "        if building in self.special_buildings:\n",
    "            folder_path = os.path.join(OUTPUT_PATH, 'special', folder_name_date, 'tsnmf')\n",
    "        else:\n",
    "            folder_path = os.path.join(OUTPUT_PATH, folder_name_date, 'tsnmf')\n",
    "            \n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        file_path = os.path.join(folder_path, f'{building}.png') \n",
    "        plt.savefig(file_path)\n",
    "        plt.close()\n",
    "\n",
    "    def apply_method(self, data_matrix):\n",
    "        return self.decomp.apply_tsnmf(data_matrix)\n",
    "    \n",
    "    def plot_multiple(self):\n",
    "        for building in self.data.keys():\n",
    "            day_data_matrix = self.data_matrix(building)\n",
    "            W, H, _ = self.apply_method(day_data_matrix)\n",
    "            self.plot_and_save(day_data_matrix, building)\n",
    "\n",
    "    def plot_approximation_all_buildings(self):\n",
    "        for building in self.data.keys():\n",
    "            day_data_matrix = self.data_matrix(building)\n",
    "            W, H, _ = self.apply_method(day_data_matrix)\n",
    "            approximation = np.dot(W, H)\n",
    "            approx_df = pd.DataFrame(approximation, \n",
    "                                     index = day_data_matrix.index, \n",
    "\n",
    "                                     columns = day_data_matrix.columns)\n",
    "            approx_series = approx_df.unstack()\n",
    "\n",
    "    def plot_and_save_all_buildings(self):\n",
    "        self.plot_multiple()\n",
    "        \n",
    "\n",
    "data_processor  = dp.DataProcessor(csv_directory='./data/input/WiFiData/', building_id='AERO')\n",
    "data_visualizer = DecompositionVisualizer(data_processor)\n",
    "data_visualizer.plot_multiple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building: ADEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping at iteration set in max_iter.\n"
     ]
    }
   ],
   "source": [
    "from src.python.utils import *\n",
    "import src.python.DataProcessor as dp\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "class DecompositionMethods:\n",
    "    \"\"\"\n",
    "    A class to process SVD, TruncatedSVD, tsnmf, and NMF methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=4):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def apply_svd(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "        U, s, VT = np.linalg.svd(data_2d)\n",
    "        S = np.zeros(data_2d.shape)\n",
    "        S[:self.n_components, :self.n_components] = np.diag(s[:self.n_components])\n",
    "        W = U @ S\n",
    "        H = VT  \n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "        return W, H, objective\n",
    "        \n",
    "    def apply_truncated_svd(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "        model = TruncatedSVD(n_components=self.n_components)\n",
    "        model.fit(data_2d)\n",
    "        W = model.transform(data_2d)\n",
    "        H = model.components_\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "        return W, H, objective\n",
    "    \n",
    "    def apply_nmf_sklearn(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "        model = NMF(n_components=self.n_components, init='random', random_state=0)\n",
    "        W = model.fit_transform(data_2d)\n",
    "        H = model.components_\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "        return W, H, objective\n",
    "\n",
    "    def apply_tsnmf(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "        model = tsnmf.smoothNMF(n_components=self.n_components)\n",
    "        model.fit(data_2d)\n",
    "        W = model.W\n",
    "        H = model.H\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "        return W, H, objective\n",
    "\n",
    "class DecompositionVisualizer:\n",
    "\n",
    "    def __init__(self, data_processor):\n",
    "        self.decomp = DecompositionMethods()\n",
    "        self.special_buildings = ['AERO', 'ATLS', 'C4C', 'LIBR', 'UMC', 'WLAW', 'REC', 'BCAPA', 'BCAPB']\n",
    "        self.data = data_processor.process_all_buildings()\n",
    " \n",
    "    def data_matrix(self, building):\n",
    "        df  =   self.data[building]\n",
    "        df[ 'datetime'  ]   =   pd.to_datetime( df[  'datetime'  ]  )\n",
    "        df[ 'date'      ]   =   df[ 'datetime'  ].dt.date\n",
    "        df[ 'time'      ]   =   df[ 'datetime'  ].dt.time\n",
    "        df[ 'hours'     ]   =   df[ 'time'].apply(lambda t: (t.hour*3600 + t.minute*60 + t.second)/3600)\n",
    "        df  =   df.groupby(['date', 'hours'])['devicecount'].mean().unstack()\n",
    "        return df\n",
    "\n",
    "    def plot_and_save(self, data_matrix, W, H, building, method):\n",
    "        print(f\"Building: {building}\")\n",
    "        \n",
    "        start_date = data_matrix.index.min().strftime('%b-%d-%Y')\n",
    "        end_date = data_matrix.index.max().strftime('%b-%d-%Y')\n",
    "        folder_name_date = f'{start_date}_to_{end_date}'\n",
    "        folder_name_interval = method\n",
    "        building_title = building + f\": {start_date} to {end_date}\"\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 20))\n",
    "        gs = gridspec.GridSpec(3, 3, height_ratios=[1, 0.5, 0.5]) \n",
    "    \n",
    "        plt.suptitle(building_title, fontsize=14, fontweight='bold')\n",
    "\n",
    "        ax0 = plt.subplot(gs[0, :])\n",
    "        ax1 = plt.subplot(gs[1, 0])\n",
    "        ax2 = plt.subplot(gs[2, 0])\n",
    "        ax3 = plt.subplot(gs[1, 1])\n",
    "        ax4 = plt.subplot(gs[2, 1])\n",
    "        ax5 = plt.subplot(gs[1, 2])\n",
    "        ax6 = plt.subplot(gs[2, 2])\n",
    "\n",
    "        for index, dayData in data_matrix.iterrows():\n",
    "            ax0.plot(dayData.index, dayData, label=index, color='blue')\n",
    "\n",
    "        ax0.xaxis.set_major_locator(ticker.MultipleLocator(4))\n",
    "        ax0.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
    "        ax0.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{int(x):02d}:{int(60*(x%1)):02d}'))\n",
    "        \n",
    "        ax0.set_xlim([0, 24])\n",
    "        ax0.set_xlabel('Time of Day')\n",
    "        ax0.set_ylabel('Device Count')\n",
    "        ax0.set_title('ORIGINAL DATA - Device Count per day')\n",
    "        ax0.legend()\n",
    "            \n",
    "        # Truncated SVD\n",
    "        W_svd, H_svd, _ = self.decomp.apply_truncated_svd(data_matrix)\n",
    "        ax1.plot(W_svd)\n",
    "        ax1.set_title('W - Truncated SVD')\n",
    "        ax2.plot(H_svd.T)\n",
    "        ax2.set_title('H - Truncated SVD')\n",
    "\n",
    "        # NMF\n",
    "        W_nmf, H_nmf, _ = self.decomp.apply_nmf_sklearn(data_matrix)\n",
    "        ax3.plot(W_nmf)\n",
    "        ax3.set_title('W - NMF')\n",
    "        ax4.plot(H_nmf.T)\n",
    "        ax4.set_title('H - NMF')\n",
    "\n",
    "        # TSNMF\n",
    "        W_tsnmf, H_tsnmf, _ = self.decomp.apply_tsnmf(data_matrix)\n",
    "        ax5.plot(W_tsnmf)\n",
    "        ax5.set_title('W - TSNMF')\n",
    "        ax6.plot(H_tsnmf.T)\n",
    "        ax6.set_title('H - TSNMF')\n",
    "\n",
    "        if building in self.special_buildings:\n",
    "            folder_path = os.path.join(OUTPUT_PATH, 'special', folder_name_date, folder_name_interval)\n",
    "        else:\n",
    "            folder_path = os.path.join(OUTPUT_PATH, folder_name_date, folder_name_interval)\n",
    "            \n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        file_path = os.path.join(folder_path, f'{building}.png') \n",
    "        plt.savefig(file_path)\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "    def apply_method(self, data_matrix, method):\n",
    "        methods = {'tsnmf': self.decomp.apply_tsnmf,'nmf': self.decomp.apply_nmf_sklearn,'trunc-svd': self.decomp.apply_truncated_svd,}\n",
    "\n",
    "        if method not in methods:\n",
    "            raise ValueError(f\"Invalid method: {method}\")\n",
    "        return methods[method](data_matrix)\n",
    "    \n",
    "    def plot_multiple(self, method):\n",
    "        for building in self.data.keys():\n",
    "            day_data_matrix = self.data_matrix(building)\n",
    "            W, H, _ = self.apply_method(day_data_matrix, method)\n",
    "            self.plot_and_save(day_data_matrix, W, H, building, method)\n",
    "\n",
    "    def plot_approximation_all_buildings(self, method):\n",
    "        for building in self.data.keys():\n",
    "            day_data_matrix = self.data_matrix(building)\n",
    "            W, H, _ = self.apply_method(day_data_matrix, method)\n",
    "            approximation = np.dot(W, H)\n",
    "            approx_df = pd.DataFrame(approximation, \n",
    "                                     index = day_data_matrix.index, \n",
    "                                     columns = day_data_matrix.columns)\n",
    "            approx_series = approx_df.unstack()\n",
    "            \n",
    "    def plot_and_save_all_buildings(self):\n",
    "        methods = ['tsnmf', 'nmf', 'trunc-svd']\n",
    "        for method in methods:\n",
    "            self.plot_multiple(method)\n",
    "        \n",
    "\n",
    "\n",
    "data_processor  = dp.DataProcessor(csv_directory = \"CUT-CSV\" )\n",
    "data_visualizer = DecompositionVisualizer(data_processor)\n",
    "data_visualizer.plot_multiple('nmf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def data_matrix(self, building):\n",
    "        df  =   self.data[building]\n",
    "        df[ 'datetime'  ]   =   pd.to_datetime( df[  'datetime'  ]  )\n",
    "        df[ 'date'      ]   =   df[ 'datetime'  ].dt.date\n",
    "        df[ 'time'      ]   =   df[ 'datetime'  ].dt.time\n",
    "        df[ 'hours'     ]   =   df[ 'time'].apply(lambda t: (t.hour*3600 + t.minute*60 + t.second)/3600)\n",
    "        df  =   df.groupby(['date', 'hours'])['devicecount'].mean().unstack()\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "\n",
    "# Matrix Decompositions\n",
    "---\n",
    "\n",
    "## Summary \n",
    "\n",
    "\n",
    "INITIALIZATION FOR NONNEGATIVE MATRIX FACTORIZATION: A COMPREHENSIVE REVIEW https://arxiv.org/pdf/2109.03874.pdf\n",
    "\n",
    "NICA–NMF. The non-uniqueness (non-convexity)\n",
    "property of NMF implies that the solution depends on the\n",
    "initial factor matrices. To solve this problem we implement the\n",
    "idea presented by Kitamura & Ono (2016) which suggests that\n",
    "a good initialization is based on the factorization given by nonnegative ICA\n",
    "\n",
    "The source above shows that NICA provideds best performance, so we want to show better performance and interpretability.\n",
    "\n",
    "Nonnegative ICA: \n",
    "    - https://ieeexplore.ieee.org/document/1015161?denied=\n",
    "    - https://github.com/Marius1311/Non-negative-ICA\n",
    "    - \n",
    "\n",
    "Other useful references:\n",
    "    - https://ieeexplore.ieee.org/document/759424\n",
    "    - https://www.hindawi.com/journals/aav/2017/7132038/\n",
    "    - https://onlinelibrary.wiley.com/doi/pdf/10.1107/S1600577523001674\n",
    "\n",
    "\n",
    "Blind source separation. Blind source separation\n",
    "(BSS) comprises all techniques that try to decouple a set of\n",
    "source signals from a set of mixed signals with unknown (or\n",
    "very little) information (Herault et al., 1985). Depending on\n",
    "the assumptions on the data, different BSS techniques can\n",
    "be used.\n",
    "\n",
    "\n",
    "- The paper using PALM is available here: https://arxiv.org/abs/1910.14576\n",
    "\n",
    "The class I made applies the time-series-nmf (TSNMF) library, a Python package specifically designed for time-series data that implements Non-negative Matrix Factorization (NMF). The TSNMF library supports Tikhonov regularization and sparse constraints and uses a Proximal Alternating Linearized Minimization (PALM) optimization scheme. The smoothNMF class in the TSNMF package implements NMF with a smoothness constraint, which is particularly helpful for time-series data, since adjacent time points are similar.\n",
    "\n",
    "- The grant information for the Python package is available here: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1849930&HistoricalAwards=false\n",
    "- Information on the package are available at these two links: https://pypi.org/project/time-series-nmf/ and https://github.com/valentina-s/time-series-nmf/tree/v0.1.0.dev0\n",
    "\n",
    "Within the smoothNMF class, `n_components` is a parameter that fixes the number of basis vectors the NMF algorithm will attempt to identify in the data. For example, in the current code, n_components is set to 5, indicating that the algorithm is seeking 5 basis vectors, also known as underlying patterns, in the device count dataset. The `fit` method in the smoothNMF class applies the NMF algorithm to the selected time interval. The `W` and `H` are attributes of the object created by the smoothNMF class--these represent the basis vectors and coefficients respectively. So, we are identifying underlying patterns in the data that are smooth as they move across time, and W and H matrices represent these patterns and their contributions to the original data over time. \n",
    "\n",
    "1. Since W represents the basis vectors (... or the patterns in the time-series data) with each line in the plot representing a basis vector (pattern), the vertical scale for W represents the magnitude of these patterns.\n",
    "\n",
    "2. Since H represents the coefficients (... or the extent to which each pattern contributes to the original dat), each line in the plot represents the weight of a basis vector over time, and the vertical scale for H represents the magnitude of these contributions.\n",
    "\n",
    "3. Recall that the time-series-nmf library applies NMF with a smoothness constraint. A smoothness constraint in time series analysis is a technique used to help identify trends and patterns in the data by reducing short-term fluctuations or noise. The idea is to make the time series smoother so that the underlying pattern is easier to see and understand. Equivallently, applying a smoothness constraint means that the algorithm will try to find basis vectors (W) and coefficients (H) that vary smoothly over time. So, the values of W and H at adjacent time points will be similar. This is particularly useful for time-series data, where the underlying patterns in the data are likely to change on a less-fast time scale. The patterns happen gradually rather than abruptly, and we observe that on a different time scale. The horizontal scale might seem to be all over the place due to this constraint, but the horizontal scales for both W and H represent time. The smoothness constraint improves the interpretability of the results, as the patterns identified by the NMF will be smoother and potentially easier to understand, and this will be helpful when we look at the \"events happening within events\" that we can see in the UMC pattern of life.\n",
    "\n",
    "4. For the original data, the vertical scale is indeed device count, and the horizontal scale represents the specific timestamps the device count happens at.\n",
    "\n",
    "Again, I am claiming that the time-series-nmf library applies this smoothness constraint when performing the NMF on the data. This is result in the horizontal scale of the W and H plots appearing to be \"all over the place\", as the smoothness constraint can cause the basis vectors and coefficients to change at a different rate over time. We just talked about something like this in my Computational Neuroscience class. I think, if this is true, then this would still work fine even if the original data has abrupt changes or is unevenly spaced. They are not unevenly spaced because of the interpolation but I did use a package to replace a lot of non-numeric entries with mean-valued entries. \n",
    "\n",
    "The Proximal Alternating Linearized Minimization (PALM) algorithm is a versatile method that can handle a broad class of nonconvex and nonsmooth minimization problems, making it a suitable choice for implementing the smoothness constraint in Non-negative Matrix Factorization (NMF) for time-series data. It is designed to solve a broad range of optimization problems with non-differentiable constraints. This makes it particularly well-suited for complex time-series data, which has high temporal variability (the change between time slices show significant differences even over short slices) and transient features (patterns that appear over time but disappear) that are usually very difficult to model with any accuracy.\n",
    "\n",
    "- The article for the PALM algorithm: https://link.springer.com/article/10.1007/s10107-013-0701-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "class DecompositionMethods:\n",
    "    \"\"\"\n",
    "    A class to process SVD, TruncatedSVD, tsnmf, and NMF methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=4):\n",
    "        self.n_components = n_components\n",
    "        self.nica = nica.NonnegativeICA()\n",
    "\n",
    "    def apply_svd(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "\n",
    "        U, s, VT = np.linalg.svd(data_2d)\n",
    "\n",
    "        S = np.zeros(data_2d.shape)\n",
    "        S[:self.n_components, :self.n_components] = np.diag(s[:self.n_components])\n",
    "\n",
    "        W = U @ S\n",
    "        H = VT\n",
    "\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "\n",
    "        return W, H, objective\n",
    "        \n",
    "    def apply_truncated_svd(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "\n",
    "        model = TruncatedSVD(n_components=self.n_components)\n",
    "        model.fit(data_2d)\n",
    "\n",
    "        W = model.transform(data_2d)\n",
    "        H = model.components_\n",
    "\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "        return W, H, objective\n",
    "    \n",
    "    def apply_nmf_sklearn(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "\n",
    "        model = NMF(n_components=self.n_components, init='random', random_state=0)\n",
    "        W = model.fit_transform(data_2d)\n",
    "        H = model.components_\n",
    "\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "        return W, H, objective\n",
    "\n",
    "    def apply_tsnmf(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "\n",
    "        model = tsnmf.smoothNMF(n_components=self.n_components)\n",
    "        model.fit(data_2d)\n",
    "\n",
    "        W = model.W\n",
    "        H = model.H\n",
    "\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "        return W, H, objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (2409228586.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 29\u001b[0;36m\u001b[0m\n\u001b[0;31m    axs[0].plot(dayData.index, dayData, label=index, color='blue)\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "class DecompositionVisualizer:\n",
    "\n",
    "    def __init__(self, data_processor):\n",
    "        self.decomp = DecompositionMethods()\n",
    "        self.special_buildings = ['AERO', 'ATLS', 'C4C', 'LIBR', 'UMC', 'WLAW', 'REC']\n",
    "        self.data = data_processor.process_all_buildings()\n",
    " \n",
    "    def data_matrix(self, building):\n",
    "        df  =   self.data[building]\n",
    "        df[ 'datetime'  ]   =   pd.to_datetime( df[  'datetime'  ]  )\n",
    "        df[ 'date'      ]   =   df[ 'datetime'  ].dt.date\n",
    "        df[ 'time'      ]   =   df[ 'datetime'  ].dt.time\n",
    "        df[ 'hours'     ]   =   df[ 'time'].apply(lambda t: (t.hour*3600 + t.minute*60 + t.second)/3600)\n",
    "        df  =   df.groupby(['date', 'hours'])['devicecount'].mean().unstack()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def plot_and_save(self, data_matrix, W, H, building, method):\n",
    "        \n",
    "        print(f\"Building: {building}\")\n",
    "        fig, axs = plt.subplots(3, 1, figsize=(15, 24))\n",
    "\n",
    "        start_date = data_matrix.index.min().strftime( '%m-%Y' )\n",
    "        end_date = data_matrix.index.max().strftime( '%m-%Y' )\n",
    "        building_title = building + f\": {start_date}_to_{end_date}\"\n",
    "        plt.suptitle(building_title, fontsize=14, fontweight='bold')\n",
    "\n",
    "        for index, dayData in data_matrix.iterrows():\n",
    "            axs[0].plot(dayData.index, dayData, label=index, color='blue)\n",
    "\n",
    "        ax = axs[0]\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(4))\n",
    "        ax.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
    "        ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{int(x):02d}:{int(60*(x%1)):02d}'))\n",
    "        \n",
    "        axs[0].set_xlim([0, 24])\n",
    "        axs[0].set_xlabel('Time of Day')\n",
    "        axs[0].set_ylabel('Device Count')\n",
    "        axs[0].set_title('ORIGINAL DATA - Device Count per day')\n",
    "        axs[0].legend()\n",
    "            \n",
    "        axs[1].plot(W)\n",
    "        axs[1].set_title('W')\n",
    "        axs[2].plot(H.T)\n",
    "        axs[2].set_title('H')\n",
    "\n",
    "        start_date = data_matrix.index.min().strftime('%b-%d-%Y')\n",
    "        end_date = data_matrix.index.max().strftime('%b-%d-%Y')\n",
    "        folder_name_date = f'{start_date}_to_{end_date}'\n",
    "        folder_name_interval = method\n",
    "\n",
    "        if building in self.special_buildings:\n",
    "            folder_path = os.path.join(OUTPUT_PATH, 'special', folder_name_date, folder_name_interval)\n",
    "        else:\n",
    "            folder_path = os.path.join(OUTPUT_PATH, folder_name_date, folder_name_interval)\n",
    "            \n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        file_path = os.path.join(folder_path, f'{building}.png') \n",
    "        plt.savefig(file_path)\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "    def apply_method(self, data_matrix, method):\n",
    "        methods = {\n",
    "            'tsnmf': self.decomp.apply_tsnmf,\n",
    "            'nmf': self.decomp.apply_nmf_sklearn,\n",
    "            'trunc-svd': self.decomp.apply_truncated_svd,\n",
    "            'svd': self.decomp.apply_svd\n",
    "        }\n",
    "        \n",
    "        if method not in methods:\n",
    "            raise ValueError(f\"Invalid method: {method}\")\n",
    "\n",
    "        return methods[method](data_matrix)\n",
    "    \n",
    "    def plot_multiple(self, method):\n",
    "        for building in self.data.keys():\n",
    "            day_data_matrix = self.data_matrix(building)\n",
    "            W, H, _ = self.apply_method(day_data_matrix, method)\n",
    "            self.plot_and_save(day_data_matrix, W, H, building, method)\n",
    "\n",
    "    def plot_approximation_all_buildings(self, method):\n",
    "        for building in self.data.keys():\n",
    "            day_data_matrix = self.data_matrix(building)\n",
    "            W, H, _ = self.apply_method(day_data_matrix, method)\n",
    "            approximation = np.dot(W, H)\n",
    "            approx_df = pd.DataFrame(approximation, \n",
    "                                     index = day_data_matrix.index, \n",
    "                                     columns = day_data_matrix.columns)\n",
    "            approx_series = approx_df.unstack()\n",
    "            \n",
    "    def plot_and_save_all_buildings(self):\n",
    "        methods = ['tsnmf', 'svd', 'nmf', 'trunc-svd']\n",
    "        for method in methods:\n",
    "            self.plot_multiple(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor  = dp.DataProcessor(csv_directory = \"CUT-CSV\"\n",
    "                                   )\n",
    "data_visualizer = DecompositionVisualizer(data_processor)\n",
    "data_visualizer.plot_multiple('nmf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/src/python/DataProcessor.py:86: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_mat['datetime'] = pd.to_datetime(data_mat['datetime'], errors='coerce')\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building: ATLS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping at iteration set in max_iter.\n",
      "Building: UMC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping at iteration set in max_iter.\n",
      "Building: DLC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping at iteration set in max_iter.\n",
      "Building: RAMY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping at iteration set in max_iter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building: STAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping at iteration set in max_iter.\n",
      "Building: STSB\n",
      "Stopping at iteration set in max_iter.\n",
      "Building: BCAPB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/var/folders/yy/tgnx4yvs65n5w70kcy3_2b000000gn/T/ipykernel_86092/4039310750.py:159: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(20, 5)) # Two subplots only (W and H) so no need to create grid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping at iteration set in max_iter.\n",
      "Building: CASE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping at iteration set in max_iter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building: KTCH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping at iteration set in max_iter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building: ITLL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerreiser/Desktop/dev/Smart-Campus/env/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.python.utils import *\n",
    "import src.python.DataProcessor as dp\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "class DecompositionMethods:\n",
    "    \"\"\"\n",
    "    A class to process SVD, TruncatedSVD, tsnmf, and NMF methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=4):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def apply_svd(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "\n",
    "        U, s, VT = np.linalg.svd(data_2d)\n",
    "\n",
    "        S = np.zeros(data_2d.shape)\n",
    "        S[:self.n_components, :self.n_components] = np.diag(s[:self.n_components])\n",
    "\n",
    "        W = U @ S\n",
    "        H = VT\n",
    "\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "\n",
    "        return W, H, objective\n",
    "        \n",
    "    def apply_truncated_svd(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "\n",
    "        model = TruncatedSVD(n_components=self.n_components)\n",
    "        model.fit(data_2d)\n",
    "\n",
    "        W = model.transform(data_2d)\n",
    "        H = model.components_\n",
    "\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "        return W, H, objective\n",
    "    \n",
    "    def apply_nmf_sklearn(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "\n",
    "        model = NMF(n_components=self.n_components, init='random', random_state=0)\n",
    "        W = model.fit_transform(data_2d)\n",
    "        H = model.components_\n",
    "\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "        return W, H, objective\n",
    "\n",
    "    def apply_tsnmf(self, data):\n",
    "        data = data.fillna(data.mean())\n",
    "        data_2d = data.values\n",
    "\n",
    "        model = tsnmf.smoothNMF(n_components=self.n_components)\n",
    "        model.fit(data_2d)\n",
    "\n",
    "        W = model.W\n",
    "        H = model.H\n",
    "\n",
    "        approximation = np.dot(W, H)\n",
    "        objective = np.linalg.norm(data_2d - approximation, 'fro')\n",
    "        return W, H, objective\n",
    "    \n",
    "    \n",
    "class DecompositionVisualizer:\n",
    "    \"\"\"\n",
    "    A class to visualize SVD, TruncatedSVD, tsnmf, and NMF methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_processor):\n",
    "        self.decomp = DecompositionMethods()\n",
    "        self.special_buildings = ['AERO', 'ATLS', 'C4C', 'LIBR', 'UMC', 'WLAW', 'REC', 'BCAPA', 'BCAPB']\n",
    "        self.data = data_processor.process_all_buildings()\n",
    " \n",
    "    def data_matrix(self, building):\n",
    "        df  =   self.data[building]\n",
    "        df[ 'datetime'  ]   =   pd.to_datetime( df[  'datetime'  ]  )\n",
    "        df[ 'date'      ]   =   df[ 'datetime'  ].dt.date\n",
    "        df[ 'time'      ]   =   df[ 'datetime'  ].dt.time\n",
    "        df[ 'hours'     ]   =   df[ 'time'].apply(lambda t: (t.hour*3600 + t.minute*60 + t.second)/3600)\n",
    "        df  =   df.groupby(['date', 'hours'])['devicecount'].mean().unstack()\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def split_df_into_periods(df, num_periods):\n",
    "        num_days        = df['date'].nunique()\n",
    "        size, remainder = divmod(num_days, num_periods)\n",
    "        sizes           = [size+1] * remainder + [size] * (num_periods - remainder)\n",
    "        unique_dates    = df['date'].unique()\n",
    "        unique_dates.sort()\n",
    "        split_dates     = np.array_split(unique_dates, sizes)\n",
    "        dataframes      = [df[df['date'].isin(dates)] for dates in split_dates]\n",
    "\n",
    "        return dataframes\n",
    "\n",
    "    def plot_and_save(self, data_matrix, W, H, building, method):\n",
    "            print(f\"Building: {building}\")\n",
    "            \n",
    "            start_date = data_matrix.index.min().strftime('%b-%d-%Y')\n",
    "            end_date = data_matrix.index.max().strftime('%b-%d-%Y')\n",
    "            folder_name_date = f'{start_date}_to_{end_date}'\n",
    "            folder_name_interval = method\n",
    "            building_title = building + f\": {start_date} to {end_date}\"\n",
    "            \n",
    "            fig = plt.figure(figsize=(20, 10))\n",
    "            gs = gridspec.GridSpec(2, 2, height_ratios=[1, 0.5]) \n",
    "\n",
    "            plt.suptitle(building_title, fontsize=14, fontweight='bold')\n",
    "\n",
    "            ax0 = plt.subplot(gs[0, :])\n",
    "            ax1 = plt.subplot(gs[1, 0])\n",
    "            ax2 = plt.subplot(gs[1, 1])\n",
    "\n",
    "            for index, dayData in data_matrix.iterrows():\n",
    "                ax0.plot(dayData.index, dayData, label=index, color='blue')\n",
    "\n",
    "            ax0.xaxis.set_major_locator(ticker.MultipleLocator(4))\n",
    "            ax0.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
    "            ax0.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{int(x):02d}:{int(60*(x%1)):02d}'))\n",
    "            \n",
    "            ax0.set_xlim([0, 24])\n",
    "            ax0.set_xlabel('Time of Day')\n",
    "            ax0.set_ylabel('Device Count')\n",
    "            ax0.set_title('ORIGINAL DATA - Device Count per day')\n",
    "            ax0.legend()\n",
    "                \n",
    "            # Truncated SVD\n",
    "            W_svd, H_svd, _ = self.decomp.apply_truncated_svd(data_matrix)\n",
    "            ax1.plot(W_svd)\n",
    "            ax1.set_title('W - Truncated SVD')\n",
    "            ax2.plot(H_svd.T)\n",
    "            ax2.set_title('H - Truncated SVD')\n",
    "            \n",
    "            plt.savefig(f\"{building}_{method}_tsvd.png\")\n",
    "\n",
    "            # NMF\n",
    "            plt.figure(figsize=(20, 5)) # Two subplots only (W and H) so no need to create grid\n",
    "            ax1 = plt.subplot(1, 2, 1)\n",
    "            ax2 = plt.subplot(1, 2, 2)\n",
    "\n",
    "            W_nmf, H_nmf, _ = self.decomp.apply_nmf_sklearn(data_matrix)\n",
    "            ax1.plot(W_nmf)\n",
    "            ax1.set_title('W - NMF')\n",
    "            ax2.plot(H_nmf.T)\n",
    "            ax2.set_title('H - NMF')\n",
    "            \n",
    "            plt.savefig(f\"{building}_{method}_nmf.png\")\n",
    "\n",
    "            # TSNMF\n",
    "            plt.figure(figsize=(20, 5)) # Two subplots only (W and H) so no need to create grid\n",
    "            ax1 = plt.subplot(1, 2, 1)\n",
    "            ax2 = plt.subplot(1, 2, 2)\n",
    "            W_tsnmf, H_tsnmf, _ = self.decomp.apply_tsnmf(data_matrix)\n",
    "            ax1.plot(W_tsnmf)\n",
    "            ax1.set_title('W - TSNMF')\n",
    "            ax2.plot(H_tsnmf.T)\n",
    "            ax2.set_title('H - TSNMF')\n",
    "\n",
    "            plt.savefig(f\"{building}_{method}_tsnmf.png\")\n",
    "\n",
    "        \n",
    "    def apply_method(self, data_matrix, method):\n",
    "        methods = {\n",
    "            'tsnmf': self.decomp.apply_tsnmf,\n",
    "            'nmf': self.decomp.apply_nmf_sklearn,\n",
    "            'trunc-svd': self.decomp.apply_truncated_svd,\n",
    "            'svd': self.decomp.apply_svd\n",
    "        }\n",
    "        \n",
    "        if method not in methods:\n",
    "            raise ValueError(f\"Invalid method: {method}\")\n",
    "\n",
    "        W, H, _ = methods[method](data_matrix)\n",
    "        \n",
    "        return W, H, _  \n",
    "    \n",
    "    def plot_multiple(self, method):\n",
    "        for building in self.data.keys():\n",
    "            day_data_matrix = self.data_matrix(building)\n",
    "            W, H, _ = self.apply_method(day_data_matrix, method)\n",
    "            self.plot_and_save(day_data_matrix, W, H, building, method)\n",
    "            \n",
    "    def plot_and_save_all_buildings(self):\n",
    "        methods = ['tsnmf', 'nmf', 'trunc-svd']\n",
    "        for method in methods:\n",
    "            self.plot_multiple(method)\n",
    "        \n",
    "\n",
    "data_processor  = dp.DataProcessor(csv_directory   =   './data/input/CUT-CSV/'\n",
    "                                   )\n",
    "data_visualizer = DecompositionVisualizer(data_processor)\n",
    "data_visualizer.plot_multiple('nmf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Plot all document spaces in 3D\n",
    "def plot3D(var):\n",
    "    n      = len(data)\n",
    "    x      = np.arange(0, n)\n",
    "    labels = ls.labels\n",
    "    fig    = plt.figure(figsize=(10,10))\n",
    "    ax     = fig.add_subplot(projection=\"3d\")\n",
    "\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(labels)\n",
    "    \n",
    "    ax.set_xlabel('Dimension', size=10)\n",
    "    ax.set_ylabel('Document' , size=10)\n",
    "    ax.set_zlabel('Magnitude', size=10)\n",
    "    \n",
    "    ax.view_init(elev=15., azim=-125)\n",
    "     \n",
    "    for i in range(var):\n",
    "        ax.plot(x, data.T[i], i, \n",
    "                zdir      = 'y',\n",
    "                linewidth = 0.6 )\n",
    "\n",
    "plot3D(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noteworthy mentions:\n",
    " \n",
    "1. Regularized Lotka-Volterra Dynamical System as Continuous Proximal-Like Method in Optimization\n",
    "    - https://link.springer.com/article/10.1023/B:JOTA.0000037603.51578.45\n",
    "\n",
    "2. A Proximal Minimization Algorithm for Structured Nonconvex and Nonsmooth Problems\n",
    "    - https://epubs.siam.org/doi/abs/10.1137/18M1190689?journalCode=sjope8\n",
    "\n",
    "3. On gradients of functions definable in o-minimal structures\n",
    "    - https://eudml.org/doc/75302\n",
    "\n",
    "4. A Decision Method for Elementary Algebra and Geometry:\n",
    "    - https://www.rand.org/content/dam/rand/pubs/reports/2008/R109.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
